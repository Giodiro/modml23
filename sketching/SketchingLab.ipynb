{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c15ef0eb",
   "metadata": {},
   "source": [
    "# LAB 1: Sketching\n",
    "Authors: \n",
    "\n",
    "    Antoine Chatalic (antoine.chatalic@dibris.unige.it)\n",
    "    \n",
    "    Giacomo Meanti  (giacomo.meanti@gmail.com)\n",
    "\n",
    "\n",
    " - This lab addresses sketching as a way to make kernel ridge regression more efficient. We will look at the Nystrom approximation and Random Fourier Features.\n",
    " - The aim of the lab is to play with the libraries and to get a practical grasp of what was discussed in class.\n",
    " - Follow the instructions below.\n",
    " \n",
    " \n",
    "**Goal**\n",
    "This lab is divided in three parts depending of their level of complexity (Beginner,\n",
    "Intermediate, Advanced). Your goal is to complete entirely, at least, one of the\n",
    "three parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b770ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "from lab_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e94403",
   "metadata": {},
   "source": [
    "## The Kernel Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aca9d2",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "Here a non-linear dataset is created with two classes and two dimensions. In this first section only the `X` variable will be used, and the class labels can be mostly ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a60f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_random_data(n_samples=1000, noise_level=.1, dataset=\"moons\")\n",
    "print(\"%d samples, %d features\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24964611",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A few data points: {X[:3]}\")\n",
    "print(f\"A few labels: {y[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d844c",
   "metadata": {},
   "source": [
    "### Compute the full kernel\n",
    "\n",
    "You will use the Gaussian kernel, which depends on the bandwidth hyperparameter ($\\sigma$)\n",
    "$$\n",
    "    k(x_1, x_2) = \\exp\\Big(-\\frac{1}{2\\sigma^2} \\lVert x_1 - x_2 \\rVert^2 \\Big)\n",
    "$$\n",
    "\n",
    "To compute the kernel matrix, we need to calculate the kernel function on every pair of points in `X`. \n",
    "We will use the [`pairwise_kernels`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html) function of sklearn for this task.\n",
    "\n",
    "The first step is to write the kernel function, following the formula above. The function takes as inputs two points $x_1$ and $x_2$ from the dataset (both are in $\\mathbb{R}^2$) and outputs a single number $k(x_1, x_2)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1, x2):\n",
    "    sigma = 1.0\n",
    "    # TODO: Fill in the Gaussian kernel formula.\n",
    "    #       You will need functions `np.exp` and `np.linalg.norm`\n",
    "    return np.exp((-1 / (2 * sigma ** 2)) * (np.linalg.norm(x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a8e82",
   "metadata": {},
   "source": [
    "Then the full kernel matrix is computed by passing the function you have defined to sklearn.\n",
    "\n",
    "We are particularly interested in answering the following question:\n",
    " - *How long does it take to compute the full kernel?*\n",
    " \n",
    "In python we can **time our code** by measuring the time before and after the code runs, and taking the difference:\n",
    "```python\n",
    "t_s = time.time()\n",
    "### our code here ###\n",
    "t_e = time.time()\n",
    "elapsed_time = t_e - t_s\n",
    "```\n",
    "\n",
    "Try to do the same to measure the time to compute the full kernel below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert timing code\n",
    "t_s = time.time()\n",
    "\n",
    "# Compute the kernel (`pairwise_kernels` is a scipy function.)\n",
    "full_kernel = pairwise_kernels(X, X, metric=gaussian_kernel)\n",
    "\n",
    "# TODO: insert timing code\n",
    "t_e = time.time()\n",
    "\n",
    "full_kernel_time = t_e - t_s\n",
    "print(f\"Computing the kernel matrix (of size {full_kernel.shape}) completed in {full_kernel_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86839d9d",
   "metadata": {},
   "source": [
    "The kernel matrix is computed between **every pair of input points**.\n",
    "Its entries indicate the similarity between such points: higher values = higher similarity and viceversa.\n",
    "\n",
    "By plotting the kernel matrix, the separation of the dataset into two classes is very clear: the first and second halves of the data belong to the first and second classes respectively. \n",
    "They have high internal similarity (similarity between points of the same class) and low external similarity (similarity to points of a different class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb98784",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(full_kernel)\n",
    "fig.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ae047",
   "metadata": {},
   "source": [
    "### Nystrom Sketching of the Kernel\n",
    "\n",
    "Now for efficiency reasons, we want to avoid computing the whole kernel matrix. In particular, assume the following situation\n",
    "$$\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "A & S \\\\\n",
    "S^\\top & Q\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "We compute $A$ and $S$, but not $Q$. \n",
    "The Nystrom method tells us how to approximate the full kernel matrix using just $A$ and $S$:\n",
    "$$\n",
    "    K \\approx \\big[A \\vert S\\big]^\\top A^{-1} \\big[A \\vert S\\big] = \\tilde{K}\n",
    "$$\n",
    "where $\\big[A \\vert S\\big]$ indicates the *concatenation* of matrices $A$ and $S$.\n",
    "\n",
    "$A$ in particular will be a small matrix, so inverting it is fast, and we will see that computing the Nystrom approximation $\\tilde{K}$ is much faster than computing $K$ exactly.\n",
    "\n",
    "\n",
    "**Algorithm:**\n",
    "You will follow these steps to implement the Nystrom approximation\n",
    " 1. Shuffle the whole dataset, in order to obtain an unbiased sampling of the Nystrom centers\n",
    " 2. Pick the Nystrom centers (the points with which $A$ is computed) by taking the first `m` points from the shuffled dataset `X`\n",
    " 2. Compute the kernel between the Nystrom centers: `A`\n",
    " 3. Compute the kernel between the Nystrom centers and the whole dataset (to form $[A \\vert S]$)\n",
    " 4. Use the Nystrom approximation formula to compute the approximate kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ea518",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_ids = np.random.permutation(X.shape[0])\n",
    "X = X[shuffle_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca628a2",
   "metadata": {},
   "source": [
    "Recompute the full kernel on the shuffled dataset. It will also be reshuffled, but contain the same information as before. (This is only used to be able to compare the approximate and full kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_kernel = pairwise_kernels(X, X, metric=gaussian_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393fe2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 50\n",
    "assert m <= X.shape[0], \"Cannot take more Nystrom centers than the number of data points\"\n",
    "\n",
    "# Since the data is shuffled, to pick random Nystrom centers we can simply choose the first m points\n",
    "nystrom_centers = X[:m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270f65d",
   "metadata": {},
   "source": [
    "Kernel between Nystrom centers. Check the [`pairwise_kernels`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html) function if you're not sure how to proceed. You'll have to pass in the `gaussian_kernel` function defined earlier as the `metric` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1017b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in. \n",
    "#       You can use the same method from the first part, \n",
    "#       but change the data to be the Nystrom centers!\n",
    "A = pairwise_kernels(\n",
    "    nystrom_centers, nystrom_centers, metric=gaussian_kernel)\n",
    "print(f\"Kernel between Nystrom centers has shape {A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e9375",
   "metadata": {},
   "source": [
    "Kernel between the Nystrom centers and the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in.\n",
    "#       Now the kernel is computed between different data matrices: the nystrom centers and the full data.\n",
    "#       You will need to pass two different arguments to the `pairwise_kernels` function.\n",
    "AS = sklearn.metrics.pairwise.pairwise_kernels(\n",
    "    nystrom_centers, X, metric=gaussian_kernel)\n",
    "print(f\"Kernel between Nystrom centers and whole dataset has shape {AS.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc4fe7",
   "metadata": {},
   "source": [
    "Compute the Nystrom approximation.\n",
    "\n",
    "We need to solve a linear system ($A^{-1} \\big[A\\vert S\\big]$) for which we use the [np.linalg.solve](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in.\n",
    "#       use matrix multiplication and the `np.linalg.solve` function!\n",
    "approx_kernel = AS.T @ np.linalg.solve(A, AS)\n",
    "assert approx_kernel.shape == full_kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b3602",
   "metadata": {},
   "source": [
    "Now plot the full and approximate kernel matrices and their difference. Notice that the two kernels are practically indistinguishable!\n",
    "To be able to see the difference we must reduce the range of the third plot to between $-10^{-7}$ and $10^{-7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e091ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 3))\n",
    "im = ax[0].imshow(full_kernel)\n",
    "fig.colorbar(im, ax=ax[0])\n",
    "ax[0].set_title(\"Full\")\n",
    "im = ax[1].imshow(approx_kernel)\n",
    "fig.colorbar(im, ax=ax[1])\n",
    "ax[1].set_title(\"Approximate\")\n",
    "im = ax[2].imshow(full_kernel - approx_kernel, vmin=-1e-7, vmax=1e-7)\n",
    "fig.colorbar(im, ax=ax[2])\n",
    "ax[2].set_title(\"Difference\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe9b0c",
   "metadata": {},
   "source": [
    "#### Quantitative evaluation of the Nystrom approximation\n",
    "\n",
    "For a more scientific comparison we will put the code for the Nystrom approximation in a function, and perform the following analyses:\n",
    " - Compute the error of the Nystrom approximation with different values of `m`\n",
    " - Compare the timings of approximating the kernel vs. computing the full one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607bb51",
   "metadata": {},
   "source": [
    "The error we're interested in is the Frobenius norm of the difference between the full kernel matrix $K$ and the approximate kernel matrix $\\tilde{K}$:\n",
    "$$\n",
    "    \\lVert K - \\tilde{K} \\rVert_F = \\sqrt{\\sum_{i, j = 1}^n (K_{ij} - \\tilde{K}_{ij})^2}\n",
    "$$\n",
    "\n",
    "We will define a function to compute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e33ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_approximation_error(full, approx):\n",
    "    return np.sqrt(np.sum(np.square(full - approx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911aa991",
   "metadata": {},
   "source": [
    "We also define a function to compute the Nystrom approximation. This should be similar to your implementation above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nystrom_approx(X, m):\n",
    "    nystrom_centers = X[:m]\n",
    "    A = pairwise_kernels(nystrom_centers, nystrom_centers, metric=gaussian_kernel)\n",
    "    AS = pairwise_kernels(nystrom_centers, X, metric=gaussian_kernel)\n",
    "    approx_kernel = AS.T @ np.linalg.solve(A, AS)\n",
    "    return approx_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a0611",
   "metadata": {},
   "source": [
    "Now we iterate over several values of `m`, recording the approximation error (in Frobenius norm) and the time it took to compute the approximation.\n",
    "\n",
    "The plot below shows the error decreases quickly as m increases (note that the y axis is on a logarithmic scale).\n",
    "On the other hand, the time required to compute the Nystrom kernel is much smaller than that for the full kernel (shown in red in the right plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 150, 200]\n",
    "approx_errors = []\n",
    "approx_timings = []\n",
    "for m in m_list:\n",
    "    t_s = time.time()\n",
    "    approx_kernel = nystrom_approx(X, m)\n",
    "    t_e = time.time()\n",
    "    approx_errors.append(kernel_approximation_error(full_kernel, approx_kernel))\n",
    "    approx_timings.append(t_e - t_s)\n",
    "    print(f\"{m} centers: error={approx_errors[-1]:.2e}  time={approx_timings[-1]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(9, 3))\n",
    "ax[0].scatter(m_list, approx_errors)\n",
    "ax[0].set_xlabel(\"m\")\n",
    "ax[0].set_ylabel(\"Approximation error\")\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_title(\"Nystrom approximation\")\n",
    "ax[1].scatter(m_list, approx_timings, label=\"Nystrom approximation\")\n",
    "ax[1].set_xlabel(\"m\")\n",
    "ax[1].set_ylabel(\"Time (s)\")\n",
    "ax[1].axhline(y=full_kernel_time, label=\"Full\", c='r')\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Nystrom timings\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a2a93",
   "metadata": {},
   "source": [
    "## Sketching For Kernel Regression\n",
    "\n",
    "Here we use Nystrom sketching for the kernel ridge regression algorithm.\n",
    "KRR is briefly introduced, along with the formulas needed to implement it. The Nystrom KRR is introduced and finally an analysis of the error and timing of the two is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b910a",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression\n",
    "\n",
    "The kernel matrix is a crucial object in the kernel ridge regression (KRR) algorithm for supervised learning.\n",
    "\n",
    "KRR consists of solving a linear regression problem in a infinite dimensional reproducing kernel Hilbert space (RKHS). \n",
    "The problem is reformulated as a finite-dimensional linear system which involves the kernel matrix, where the kernel function is the one defining the specific RKHS.\n",
    "\n",
    "In practice, denote the RKHS $\\mathcal{H}$, for example defined by the Gaussian kernel we have used above. Denote the dataset by $X = \\{x_i\\}_{i=1}^n$ and $Y = \\{y_i\\}_{i=1}^n$.\n",
    "To ensure uniqueness of the solution we must add a positive regularizer, controlled by hyperparameter $\\lambda > 0$, which pushes solutions to zero.\n",
    "\n",
    "$$\n",
    " \\hat{f} = \\arg\\min_{f\\in\\mathcal{H}} \\sum_{i=1}^n (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert\n",
    "$$\n",
    "\n",
    "By the representer theorem, every $f\\in\\mathcal{H}$ can be written as \n",
    "$$\n",
    "    f(x) = \\sum_{i=1}^n \\alpha_i k(x, x_i)\n",
    "$$\n",
    "for some vector $\\alpha\\in\\mathbb{R}^n$. From this it is possible to derive the expression for the norm of $f$:\n",
    "$\\lVert f \\rVert = \\alpha^\\top K \\alpha$.\n",
    "\n",
    "Putting everything together, you can differentiate the objective and setting the derivative to zero obtain a closed-form for $\\hat{f}$. This closed-form is what we will write down in code to solve KRR:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = k(x, X) \\hat{\\alpha}, \\quad \\hat{\\alpha} = (K + \\lambda I)^{-1} Y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b62c660",
   "metadata": {},
   "source": [
    "Once again we rely on the two moons dataset, with a bit more noise this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_random_data(n_samples=2000, noise_level=.3, dataset=\"moons\")\n",
    "print(\"%d samples, %d features\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ce7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9018a",
   "metadata": {},
   "source": [
    "In order to evaluate supervised learning models, it is necessary have some **held-out data** which has not been used for training.\n",
    "We used the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from scikit-learn to create a training set and a test set, both with 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1064820",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8efb59",
   "metadata": {},
   "source": [
    "The hyperparameters for KRR need to be set a priori. Here the following values yield good results, but feel free to experiment with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08384ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_bandwidth = 1.0  # sigma in the notes\n",
    "regularization = 1e-6   # lambda in the notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ec3bc",
   "metadata": {},
   "source": [
    "The following function computes the **classification error** of a model's predictions.\n",
    "The models we consider always output a real number, and the labels in the moons dataset are always in $\\{-1, +1\\}$.\n",
    "The classification error is computed by comparing the labels to the **sign** of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classif_error(y_true, y_pred):\n",
    "    return np.mean(np.sign(y_pred) != y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510beffe",
   "metadata": {},
   "source": [
    "#### Define a class which contains the KRR model.\n",
    "\n",
    "The input parameters are:\n",
    " - the **kernel bandwidth** ($\\sigma$)\n",
    " - the amount of **regularization** ($\\lambda$)\n",
    "which we have seen above.\n",
    "\n",
    "You might notice that the way the kernel matrix is computed changes slightly: \n",
    " - we use the `\"rbf\"` metric predefined in scikit-learn \n",
    " - the metric does not use parameter $\\sigma$ but $\\gamma = 1/2\\sigma^2$.\n",
    " \n",
    "This is much faster than passing our own function, otherwise the kernel computation becomes the bottleneck.\n",
    "\n",
    "You should fill in the blanks. As before, solving a linear system can be done with the [np.linalg.solve](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html) function from numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidge:\n",
    "    def __init__(self, kernel_bandwidth, regularization):\n",
    "        self.kernel_gamma = 1 / (2 * kernel_bandwidth ** 2)\n",
    "        self.regularization = regularization\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Full kernel matrix\n",
    "        K = pairwise_kernels(\n",
    "            X, X, metric=\"rbf\", gamma=self.kernel_gamma)\n",
    "        # Regularized kernel matrix: add lambda * I\n",
    "        reg_K = K + self.regularization * np.eye(X.shape[0])\n",
    "        # TODO: Fill in!\n",
    "        #       Solve the linear system between reg_K and y\n",
    "        alpha = np.linalg.solve(reg_K, y)\n",
    "        \n",
    "        self.alpha_ = alpha\n",
    "        self.X_ = X\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Kernel between the points we wish to predict at (`X`) \n",
    "        # and the training points (`self.X_`)\n",
    "        k_predict = pairwise_kernels(\n",
    "            X, self.X_, metric=\"rbf\", gamma=self.kernel_gamma)\n",
    "        return k_predict @ self.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c2385",
   "metadata": {},
   "source": [
    "The following cell shows \n",
    " - how to initialize the model, \n",
    " - fit it to some training data,\n",
    " - make predictions\n",
    " - calculate the error\n",
    "\n",
    "A plot of the predictions shows where the model makes mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "krr_model = KernelRidge(kernel_bandwidth, regularization)\n",
    "# Fit to the training data\n",
    "krr_model.fit(X_train, y_train);\n",
    "\n",
    "# Predictions on training and test sets\n",
    "pred_train = krr_model.predict(X_train)\n",
    "pred_test = krr_model.predict(X_test)\n",
    "\n",
    "# Classification error\n",
    "krr_train_err = binary_classif_error(y_train, pred_train)\n",
    "krr_test_err = binary_classif_error(y_test, pred_test)\n",
    "\n",
    "print(\"Training error: %.2f%%\" % (krr_train_err * 100))\n",
    "print(\"Test error: %.2f%%\" % (krr_test_err * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "# Color indicates the correct class, crossed out samples are the ones which our model gets wrong.\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_test[y_test == -1][:, 0], X_test[y_test == -1][:, 1], alpha=0.5)\n",
    "ax.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], alpha=0.5)\n",
    "ax.scatter(\n",
    "    X_test[y_test != np.sign(pred_test)][:, 0],\n",
    "    X_test[y_test != np.sign(pred_test)][:, 1],\n",
    "    alpha=0.5, marker='x', color='r')\n",
    "ax.set_title(\"Test predictions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226355b",
   "metadata": {},
   "source": [
    "As before we are particularly interested in measuring the **efficiency** of the code.\n",
    "Here you should fill in the blanks to measure the time it takes to \n",
    " 1. train the model\n",
    " 2. make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859db47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the model training and predictions\n",
    "krr_model = KernelRidge(kernel_bandwidth, regularization)\n",
    "\n",
    "# TODO: Fill in (timing code)\n",
    "t_s = time.time()\n",
    "krr_model.fit(X_train, y_train)\n",
    "# TODO: Fill in (timing code)\n",
    "t_e = time.time()\n",
    "# TODO: Fill in (timing code)\n",
    "krr_fit_time = t_e - t_s\n",
    "\n",
    "# TODO: Fill in (timing code as before)\n",
    "t_s = time.time()\n",
    "krr_model.predict(X_test)\n",
    "t_e = time.time()\n",
    "krr_pred_time = t_e - t_s\n",
    "\n",
    "print(f\"KRR timings: fit={krr_fit_time:.4f}s  predict={krr_pred_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e91d7",
   "metadata": {},
   "source": [
    "### Nystrom Kernel Ridge Regression\n",
    "\n",
    "Nystrom KRR is an approximation to the full KRR model which uses the Nystrom approximation instead of building the full kernel matrix.\n",
    "\n",
    "Denote by $\\tilde{X}_m$ the *Nystrom inducing points*: the subset of the dataset which is used to build $A$ (see the first part of this lab). For convenience, we will denote the kernel between the Nystrom centers by $K_{mm}$.\n",
    "Similarly, we will denote the kernel between the Nystrom centers and the full dataset (which we called $[A \\vert S]$ before) $K_{mn}$.\n",
    "\n",
    "Then the Nystrom approximation tells us that\n",
    "$$\n",
    "    \\tilde{K} = K_{mn}^\\top K_{mm}^{-1} K_{mn} \\approx K.\n",
    "$$\n",
    "If we replace $K$ for $\\tilde{K}$ in the expressions for kernel ridge regression we obtain the Nystrom KRR estimator.\n",
    "\n",
    "<font color=\"red\">TODO: Is this true? Dimensions clearly don't match but then why are they different?</font>\n",
    "\n",
    "A more computationally efficient way of obtaining the N-KRR estimator is through an optimization problem in the space $\\mathcal{H}_m = \\mathrm{span}\\{k(\\tilde{x}_1, \\cdot), \\dots, k(\\tilde{x}_m, \\cdot)\\}$.\n",
    "\n",
    "$$\n",
    "\\tilde{f} = \\arg\\min_{f\\in\\mathcal{H}_m} \\sum_{i=1}^n (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert\n",
    "$$\n",
    "\n",
    "where the representer theorem in the new space gives (for every $f\\in\\mathcal{H}_m$)\n",
    "$$\n",
    "    f(x) = \\sum_{i=1}^m \\alpha_i k(x, \\tilde{x}_i)\n",
    "$$\n",
    "\n",
    "Once again, you can differentiate the objective and set the derivative to zero to obtain a closed-form for $\\tilde{f}$:\n",
    "$$\n",
    "\\tilde{f}(x) = k(x, \\tilde{X}_m) \\tilde{\\alpha}, \\quad \\tilde{\\alpha} = (K_{mn} K_{nm} + \\lambda K_{mm})^{-1} K_{mn} Y\n",
    "$$\n",
    "\n",
    "We will implement this formula in the `NystromKernelRidge` class below. Notice that now we must additionally take a parameter `m` indicating how many centers to pick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e22db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NystromKernelRidge:\n",
    "    def __init__(self, kernel_bandwidth, regularization, m):\n",
    "        self.kernel_gamma = 1 / (2 * kernel_bandwidth ** 2)\n",
    "        self.regularization = regularization\n",
    "        self.m = m\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Randomly pick the Nystrom centers\n",
    "        nys_ids = np.random.permutation(X.shape[0])[:self.m]\n",
    "        nys_centers = X[nys_ids]\n",
    "        \n",
    "        # Compute the kernel matrices\n",
    "        Kmm = pairwise_kernels(\n",
    "            nys_centers, nys_centers, metric=\"rbf\", gamma=self.kernel_gamma)\n",
    "        Kmn = pairwise_kernels(\n",
    "            nys_centers, X, metric=\"rbf\", gamma=self.kernel_gamma)\n",
    "        \n",
    "        # TODO: Fill in to a) build the linear system and b) solve it.\n",
    "        to_inv = Kmn @ Kmn.T + self.regularization * Kmm\n",
    "        alpha = np.linalg.solve(to_inv, Kmn @ y)\n",
    "        \n",
    "        self.alpha_ = alpha\n",
    "        self.nys_centers_ = nys_centers\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # TODO: Fill in.\n",
    "        #       This is similar to KRR, but using the Nystrom centers (`self.nys_centers`)\n",
    "        #       instead of the training set\n",
    "        k_predict = pairwise_kernels(\n",
    "            X_test, self.nys_centers_, metric=\"rbf\", gamma=self.kernel_gamma)\n",
    "        return k_predict @ self.alpha_\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7b70b",
   "metadata": {},
   "source": [
    "Use the N-KRR model to train and predict on the same data. Does the error change? By how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nkrr_model = NystromKernelRidge(kernel_bandwidth, regularization, m=100)\n",
    "nkrr_model.fit(X_train, y_train);\n",
    "\n",
    "# TODO: Fill in model predictions\n",
    "pred_train = nkrr_model.predict(X_train)\n",
    "pred_test = nkrr_model.predict(X_test)\n",
    "\n",
    "print(\"Training error: %.2f%%\" % (binary_classif_error(y_train, pred_train) * 100))\n",
    "print(\"Test error: %.2f%%\" % (binary_classif_error(y_test, pred_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c5420",
   "metadata": {},
   "source": [
    "Of course we are also interested in the efficiency. Fill in the code to time fit and predict phases of the N-KRR algorithm.\n",
    "\n",
    "How do the timings compare to what we obtained with KRR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7da8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the model training and predictions\n",
    "nkrr_model = NystromKernelRidge(kernel_bandwidth, regularization, m=100)\n",
    "\n",
    "# TODO: Fill in (timing code)\n",
    "t_s = time.time()\n",
    "nkrr_model.fit(X_train, y_train)\n",
    "t_e = time.time()\n",
    "nkrr_fit_time = t_e - t_s\n",
    "\n",
    "# TODO: Fill in (timing code)\n",
    "t_s = time.time()\n",
    "nkrr_model.predict(X_test)\n",
    "t_e = time.time()\n",
    "nkrr_pred_time = t_e - t_s\n",
    "\n",
    "print(f\"Nystrom KRR timings: fit={nkrr_fit_time:.4f}s  predict={nkrr_pred_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd536fb",
   "metadata": {},
   "source": [
    "### Quantitative Error/Time Analysis\n",
    "\n",
    "To quantify the tradeoff between faster and more accurate algorithms, we train N-KRR with different values of `m` and plot the test error and training time.\n",
    "\n",
    "Unfortunately, especially when `m` is small there can be a lot of *variance* in the results. Depending on how the Nystrom centers are chosen, you could have a good or a very bad model. \n",
    "To reduce the variance, \n",
    "**adapt the following code to run multiple repetitions and plot the mean error and time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aaec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = [2, 4, 8, 10, 15, 20, 50, 100, 200]\n",
    "nkrr_errors = []\n",
    "nkrr_timings = []\n",
    "for m in m_list:\n",
    "    # TODO: Fill in. Add code to run multiple repetitions.\n",
    "    it_err, it_time = [], []\n",
    "    for i in range(20):\n",
    "        nkrr_model = NystromKernelRidge(kernel_bandwidth, regularization, m=m)\n",
    "        t_s = time.time()\n",
    "        nkrr_model.fit(X_train, y_train)\n",
    "        pred_test = nkrr_model.predict(X_test)\n",
    "        t_e = time.time()\n",
    "        it_err.append(binary_classif_error(y_test, pred_test))\n",
    "        it_time.append(t_e - t_s)\n",
    "    # TODO: Fill in. Add code to take the mean over repetitions\n",
    "    nkrr_errors.append(np.mean(it_err))\n",
    "    nkrr_timings.append(np.mean(it_time))\n",
    "    print(f\"{m} centers: N-KRR error={nkrr_errors[-1] * 100:.1f}%  time={nkrr_timings[-1]:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee21317",
   "metadata": {},
   "source": [
    "What does the plot show? How much more efficient is N-KRR than KRR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a343a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(9, 3))\n",
    "ax[0].scatter(m_list, nkrr_errors, label=\"N-KRR\")\n",
    "ax[0].set_xlabel(\"m\")\n",
    "ax[0].set_ylabel(\"test classification error\")\n",
    "ax[0].axhline(y=krr_test_err, label=\"KRR\", c=\"r\")\n",
    "ax[0].set_xscale('log')\n",
    "\n",
    "ax[1].scatter(m_list, nkrr_timings, label=\"N-KRR\")\n",
    "ax[1].set_xlabel(\"m\")\n",
    "ax[1].set_ylabel(\"Time (s)\")\n",
    "ax[1].axhline(y=krr_fit_time + krr_pred_time, label=\"Full\", c='r')\n",
    "ax[1].legend()\n",
    "ax[1].set_xscale('log')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288dc3d2",
   "metadata": {},
   "source": [
    "## Random Fourier Features\n",
    "\n",
    "Another algorithm for efficient kernel regresion is *Random Fourier Features*.\n",
    "\n",
    "Taking a step back, every RKHS $\\mathcal{H}$ comes with a feature map $\\phi$ such that $\\phi(x)\\in\\mathcal{H}$ and $\\langle \\phi(x), \\phi(x') \\rangle_\\mathcal{H} = k(x, x')$.\n",
    "\n",
    "For common choices of RKHSs, such as the one induced by the Gaussian kernel, $\\mathcal{H}$ is infinite-dimensional and thus we cannot compute $\\phi(x)$ exactly. \n",
    "The idea of random feature expansions is to approximate $\\phi(x)$ by a finite-dimensional expansion\n",
    "$$\n",
    "    z(x) = [z_1(x), \\dots, z_r(x)]^\\top\n",
    "$$\n",
    "such that $z(x)^\\top z(x') \\approx \\phi(x)^\\top \\phi(x') = k(x, x')$. $r$ is the number of random features, and as $r$ increases the approximation will get more precise.\n",
    "\n",
    "For supervised learning the features $z(x)$ can be used instead of $x$ to train a linear regression model, which is what we will do below in the `RFFRegression` class.\n",
    "\n",
    "But first, let's have a look at $z$ for the case of the Gaussian kernel:\n",
    "$$\n",
    "z_i(x) = \\big[\\cos(x^\\top w_i) \\vert \\sin(x^\\top w_i)\\big], \\quad w_i \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "Then we need to sample $w_i$ from the standard Gaussian distribution for every random feature, multiply it by the data and take sines and cosines. If $x_i \\in \\mathbb{R}^d$, the transformed data point is $z(x_i) \\in \\mathbb{R}^{2r}$, where typically $r \\gg d$ so the dimension of the data increases. As long as $r < n$ however, solving the RFF problem remains cheaper than full KRR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f408ca4",
   "metadata": {},
   "source": [
    "As before, we generate random data from the moons dataset and split it into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_random_data(n_samples=2000, noise_level=.3, dataset=\"moons\")\n",
    "print(\"%d samples, %d features\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78119fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553278b",
   "metadata": {},
   "source": [
    "The `RFFMap` class implements random Fourier features for the Gaussian kernel. Once fitted it can be used to transform the dataset.\n",
    "\n",
    "In the `fit` method, we initialize the Gaussian weights (one per random feature requested).\n",
    "\n",
    "In the `transform` method we compute the random feature on some input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFFMap:\n",
    "    def __init__(self, num_features, kernel_bandwidth):\n",
    "        self.num_features = num_features // 2  # divide by 2 since we concatenate cos and sin features.\n",
    "        self.kernel_bandwidth = kernel_bandwidth\n",
    "    \n",
    "    def fit(self, X):\n",
    "        # These are the Gaussian weights.\n",
    "        # We divide by the kernel bandwidth to properly scale the kernel\n",
    "        self.W_ = np.random.randn(X.shape[1], self.num_features) / self.kernel_bandwidth\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X @ self.W_\n",
    "        Z = np.concatenate([np.cos(X), np.sin(X)], axis=1)\n",
    "        return Z / np.sqrt(self.num_features)  # normalization\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in a single function\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce9a51",
   "metadata": {},
   "source": [
    "As before with the Nystrom kernel we compare the RFF kernel with the full kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87600c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the full kernel. \n",
    "# Note that `gamma` and `lengthscale` are different parameterizations:\n",
    "#   gamma = 1 / (2 * lengthscale)\n",
    "full_kernel = pairwise_kernels(\n",
    "    X, X, metric='rbf', gamma=1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b63cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RFFMap class with 100 features and lengthscale equals to 1. \n",
    "# Transform the data, and compute the approximate kernel\n",
    "rff_feature = RFFMap(num_features=100, kernel_bandwidth=1)\n",
    "rf = rff_feature.fit_transform(X)\n",
    "# TODO: Fill in. How is the kernel defined with random features?\n",
    "K_rf = rf @ rf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RFF approximation error: {kernel_approximation_error(K_rf, full_kernel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(K_rf - full_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0072ff1",
   "metadata": {},
   "source": [
    "The approximation error is noticeably worse than that of Nystrom, but how will RFF fare when doing regression?\n",
    "\n",
    "In the following cell you'll have to implement the following algorithm (in the `fit` method)\n",
    " 1. Compute the features using `self.rff_map`. Note that the feature transformer must be fitted first! This will give you $Z$ from $X$.\n",
    " 2. Compute the covariance: $Z^\\top Z$\n",
    " 3. Regularize the covariance ($Z^\\top Z + \\lambda I$)\n",
    " 4. Solve the following linear system for $\\alpha$: $\\alpha = (Z^\\top Z + \\lambda I)^{-1} Z^\\top Y$\n",
    " \n",
    "The predict method is similar to what we've seen before:\n",
    " 1. Transform the data using `self.rff_map`. Note that the feature transformer **has already been fitted**. Don't fit it again.\n",
    " 2. Compute the product $Z \\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFFRegression:\n",
    "    def __init__(self, kernel_bandwidth, regularization, num_features):\n",
    "        self.regularization = regularization\n",
    "        self.rff_map = RFFMap(num_features, kernel_bandwidth)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Z = self.rff_map.fit_transform(X)\n",
    "        \n",
    "        # Linear regression on the transformed data\n",
    "        # TODO: Fill in. Follow the algorithm described above.\n",
    "        cov = Z.T @ Z\n",
    "        reg_cov = cov + self.regularization * np.eye(cov.shape[0])\n",
    "        alpha = np.linalg.solve(reg_cov, Z.T @ y)\n",
    "        \n",
    "        self.alpha_ = alpha\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Z = self.rff_map.transform(X)\n",
    "        return Z @ self.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a32c17",
   "metadata": {},
   "source": [
    "Now it's time to test the model on our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the model. We use bandwidth 1, regularization 1e-6 and 300 features.\n",
    "# Feel free to experiment with these parameters!\n",
    "rff_model = RFFRegression(1, 1e-6, num_features=300)\n",
    "rff_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in model predictions on training and test sets\n",
    "pred_train = rff_model.predict(X_train)\n",
    "pred_test = rff_model.predict(X_test)\n",
    "\n",
    "print(\"Training error: %.2f%%\" % (binary_classif_error(y_train, pred_train) * 100))\n",
    "print(\"Test error: %.2f%%\" % (binary_classif_error(y_test, pred_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a89fa8",
   "metadata": {},
   "source": [
    "Even though the kernel approximation was not great, RFF manages to obtain similar error to the full and Nystrom KRR models.\n",
    "\n",
    "As a final task, try to plot how the error and timings change as the number of random features changes. You can use a similar template as what was used in the previous parts of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1964494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Error and Timings for RFF model as the number of random features increases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
